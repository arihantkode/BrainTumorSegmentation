{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arihantkode/BrainTumorSegmentation/blob/main/unet_segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZXzgNHN8df1",
        "outputId": "a81bee3c-af58-4640-ff96-74dfc8c3b105"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nilearn\n",
            "  Downloading nilearn-0.12.1-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from nilearn) (1.5.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from nilearn) (5.4.0)\n",
            "Requirement already satisfied: nibabel>=5.2.0 in /usr/local/lib/python3.12/dist-packages (from nilearn) (5.3.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.12/dist-packages (from nilearn) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from nilearn) (25.0)\n",
            "Requirement already satisfied: pandas>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from nilearn) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.25.0 in /usr/local/lib/python3.12/dist-packages (from nilearn) (2.32.4)\n",
            "Requirement already satisfied: scikit-learn>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from nilearn) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from nilearn) (1.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6 in /usr/local/lib/python3.12/dist-packages (from nibabel>=5.2.0->nilearn) (4.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.0->nilearn) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.0->nilearn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.0->nilearn) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.25.0->nilearn) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.25.0->nilearn) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.25.0->nilearn) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.25.0->nilearn) (2025.8.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.4.0->nilearn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=2.2.0->nilearn) (1.17.0)\n",
            "Downloading nilearn-0.12.1-py3-none-any.whl (12.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nilearn\n",
            "Successfully installed nilearn-0.12.1\n"
          ]
        }
      ],
      "source": [
        "!pip install nilearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "whGdH5HV-mzk",
        "outputId": "cef05ba1-3c9d-45a1-959d-66e897f5995c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3137448955.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install scikit-learn'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m       \u001b[0m_pip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_send_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36mprint_previous_import_warning\u001b[0;34m(output)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m   \u001b[0;34m\"\"\"Prints a warning about previously imported packages.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m   \u001b[0mpackages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mpackages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# display a list of packages using the colab-display-data mimetype, which\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_previously_imported_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m   \u001b[0;34m\"\"\"List all previously imported packages from a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m   \u001b[0minstalled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_toplevel_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstalled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_extract_toplevel_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;34m\"\"\"Extract the list of toplevel packages associated with a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0mtoplevel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpackages_distributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m       \u001b[0mtoplevel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mpackages_distributions\u001b[0;34m()\u001b[0m\n\u001b[1;32m    945\u001b[0m     \u001b[0mpkg_to_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpkg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_top_level_declared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_top_level_inferred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m             \u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpkg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36m_top_level_inferred\u001b[0;34m(dist)\u001b[0m\n\u001b[1;32m    957\u001b[0m     opt_names = {\n\u001b[1;32m    958\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetmodulename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malways_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    960\u001b[0m     }\n\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mfiles\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    498\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m         return skip_missing_files(\n\u001b[0m\u001b[1;32m    501\u001b[0m             make_files(\n\u001b[1;32m    502\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_files_distinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/_functools.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(param, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mskip_missing_files\u001b[0;34m(package_paths)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mpass_none\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mskip_missing_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         return skip_missing_files(\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mpass_none\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mskip_missing_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         return skip_missing_files(\n",
            "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36mexists\u001b[0;34m(self, follow_symlinks)\u001b[0m\n\u001b[1;32m    858\u001b[0m         \"\"\"\n\u001b[1;32m    859\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    861\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ignore_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36mstat\u001b[0;34m(self, follow_symlinks)\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mdoes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m         \"\"\"\n\u001b[0;32m--> 840\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36m__fspath__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    445\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__fspath__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zKzEUS_-IdI"
      },
      "outputs": [],
      "source": [
        "!export CUDA_LAUNCH_BLOCKING=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RD88oDnJ8iVB"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XE7Bxqen8r0-"
      },
      "outputs": [],
      "source": [
        "!cp '/content/gdrive/MyDrive/Brain_Tumour_Segmentation/MICCAI_BraTS_2018_Data_Training.zip' '/content/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mg1oKmMu8shE"
      },
      "outputs": [],
      "source": [
        "!unzip /content/MICCAI_BraTS_2018_Data_Training.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mZ5bXKo8vfN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# Set the environment variable before importing any CUDA-dependent libraries\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "import torch\n",
        "import json\n",
        "import random\n",
        "import glob\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from typing import Tuple, Optional, Dict, List\n",
        "from nilearn import plotting, image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from datetime import ¸"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNa1b3H982xS"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_FLo2Bo88WL"
      },
      "outputs": [],
      "source": [
        "hgg_data_folder = '/content/MICCAI_BraTS_2018_Data_Training/training/HGG'\n",
        "t1_pattern = os.path.join(hgg_data_folder, f\"**/*t1.nii\")\n",
        "t1ce_pattern = os.path.join(hgg_data_folder, f\"**/*t1ce.nii\")\n",
        "t2_pattern = os.path.join(hgg_data_folder, f\"**/*t2.nii\")\n",
        "flair_pattern = os.path.join(hgg_data_folder, f\"**/*flair.nii\")\n",
        "seg_pattern = os.path.join(hgg_data_folder, f\"**/*seg.nii\")\n",
        "\n",
        "t1_files = glob.glob(t1_pattern)\n",
        "t1ce_files = glob.glob(t1ce_pattern)\n",
        "t2_files = glob.glob(t2_pattern)\n",
        "flair_files = glob.glob(flair_pattern)\n",
        "seg_files = glob.glob(seg_pattern)\n",
        "all_files = np.array([t1_files, t2_files, t1ce_files, flair_files, seg_files]).T\n",
        "all_files = pd.DataFrame(all_files, columns=['t1', 't2', 't1ce', 'flair', 'label'])\n",
        "\n",
        "print(f\"example: {all_files['t1'][0]}\")\n",
        "print(f\"shape of all files : {all_files.shape}\")\n",
        "print(f\"Number of t1 files: {len(t1_files)}\")\n",
        "print(f\"Number of t1ce files: {len(t1ce_files)}\")\n",
        "print(f\"Number of t2 files: {len(t2_files)}\")\n",
        "print(f\"Number of flair files: {len(flair_files)}\")\n",
        "print(f\"Number of seg files: {len(seg_files)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "syv1yE_dBIDI"
      },
      "outputs": [],
      "source": [
        "train_set, test_set = train_test_split(all_files, test_size=0.2, random_state=42)\n",
        "train_set, val_set = train_test_split(train_set, test_size=0.1, random_state=42)\n",
        "print(f\"shape of train set : {train_set.shape}\")\n",
        "print(f\"shape of test set : {test_set.shape}\")\n",
        "print(f\"shape of validation set : {val_set.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiSHW2urDaur"
      },
      "outputs": [],
      "source": [
        "t1_train = train_set['t1']\n",
        "t2_train = train_set['t2']\n",
        "t1ce_train = train_set['t1ce']\n",
        "flair_train = train_set['flair']\n",
        "labels_train = train_set['label']\n",
        "\n",
        "t1_test = test_set['t1']\n",
        "t2_test = test_set['t2']\n",
        "t1ce_test = test_set['t1ce']\n",
        "flair_test = test_set['flair']\n",
        "labels_test = test_set['label']\n",
        "\n",
        "print(f\"t1 train shape : {t1_train.shape}, t1 test shape : {t1_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVgxbS56GIEp"
      },
      "outputs": [],
      "source": [
        "t1_train_imgs = [nib.load(img).get_fdata() for img in t1_train[:20]]\n",
        "train_labels_imgs = [nib.load(img).get_fdata() for img in labels_train[:20]]\n",
        "print(f\"image shape = {train_labels_imgs[0].shape}\")\n",
        "print(f\"unique labels = {np.unique(train_labels_imgs[0])}\")\n",
        "print(f\"header = {nib.load(t1_train[0]).header}\")\n",
        "plotting.plot_roi(nib.load(labels_train[0]), nib.load(t1_train[0]), cmap='Paired')\n",
        "plt.show()\n",
        "t1_train_imgs = np.array(t1_train_imgs)\n",
        "train_labels_imgs = np.array(train_labels_imgs)\n",
        "t1_train_imgs = t1_train_imgs.reshape(-1, 240, 240)\n",
        "train_labels_imgs = train_labels_imgs.reshape(-1, 240, 240)\n",
        "print(f\"t1 train shape : {t1_train_imgs.shape}\")\n",
        "print(f\"train labels shape : {train_labels_imgs.shape}\")\n",
        "\n",
        "slice_idx = 60\n",
        "plt.imshow(t1_train_imgs[slice_idx], cmap='gray')\n",
        "plt.title(f\"t1 train slice = {slice_idx - 1}\")\n",
        "plt.show()\n",
        "plt.imshow(train_labels_imgs[slice_idx], cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hz_dvpPqS1fV"
      },
      "outputs": [],
      "source": [
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"\n",
        "    Double convolution block with batch normalization and ReLU activation.\n",
        "    This is the basic building block used throughout the UNet architecture.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels: int, out_channels: int, mid_channels: Optional[int] = None):\n",
        "        super().__init__()\n",
        "        if not mid_channels:\n",
        "            mid_channels = out_channels\n",
        "\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(mid_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.double_conv(x)\n",
        "\n",
        "\n",
        "class Down(nn.Module):\n",
        "    \"\"\"\n",
        "    Downsampling block: maxpool followed by double convolution.\n",
        "    Reduces spatial dimensions by half and increases channel depth.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels: int, out_channels: int):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "\n",
        "class Up(nn.Module):\n",
        "    \"\"\"\n",
        "    Upsampling block: upsampling followed by double convolution.\n",
        "    Increases spatial dimensions by factor of 2 and decreases channel depth.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels: int, out_channels: int, bilinear: bool = True):\n",
        "        super().__init__()\n",
        "\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
        "            self.conv = DoubleConv(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x1: torch.Tensor, x2: torch.Tensor) -> torch.Tensor:\n",
        "        x1 = self.up(x1)\n",
        "\n",
        "        # Handle input size differences\n",
        "        diff_y = x2.size()[2] - x1.size()[2]\n",
        "        diff_x = x2.size()[3] - x1.size()[3]\n",
        "\n",
        "        x1 = F.pad(x1, [diff_x // 2, diff_x - diff_x // 2,\n",
        "                        diff_y // 2, diff_y - diff_y // 2])\n",
        "\n",
        "        # Concatenate along channel dimension\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class OutConv(nn.Module):\n",
        "    \"\"\"\n",
        "    Output convolution layer that maps to the desired number of output channels.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels: int, out_channels: int):\n",
        "        super(OutConv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    \"\"\"\n",
        "    UNet architecture for brain tumor segmentation.\n",
        "\n",
        "    Architecture:\n",
        "    - Input: 240x240 MRI slice\n",
        "    - Encoder: 4 downsampling blocks with skip connections\n",
        "    - Bottleneck: Double convolution at the deepest level\n",
        "    - Decoder: 4 upsampling blocks with skip connections\n",
        "    - Output: 240x240 segmentation mask\n",
        "\n",
        "    Args:\n",
        "        n_channels: Number of input channels (default: 1 for grayscale MRI)\n",
        "        n_classes: Number of output classes (default: 4 for BraTS: background, necrosis, edema, enhancing)\n",
        "        bilinear: Whether to use bilinear upsampling (default: True)\n",
        "        dropout_rate: Dropout rate for regularization (default: 0.1)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_channels: int = 1, n_classes: int = 4, bilinear: bool = True, dropout_rate: float = 0.1, device: str = 'auto'):\n",
        "        super(UNet, self).__init__()\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.bilinear = bilinear\n",
        "\n",
        "        # Device handling\n",
        "        if device == 'auto':\n",
        "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        else:\n",
        "            self.device = torch.device(device)\n",
        "\n",
        "        # Move model to device\n",
        "        self.to(self.device)\n",
        "\n",
        "        # Encoder path (downsampling)\n",
        "        self.inc = DoubleConv(n_channels, 64)\n",
        "        self.down1 = Down(64, 128)\n",
        "        self.down2 = Down(128, 256)\n",
        "        self.down3 = Down(256, 512)\n",
        "        factor = 2 if bilinear else 1\n",
        "        self.down4 = Down(512, 1024 // factor)\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = DoubleConv(1024 // factor, 1024 // factor)\n",
        "\n",
        "        # Decoder path (upsampling)\n",
        "        # up1: bottleneck(512) + skip(512) = 1024 input channels\n",
        "        self.up1 = Up(1024 // factor + 512, 512 // factor, bilinear)\n",
        "        # up2: up1_output(256) + skip(256) = 512 input channels\n",
        "        self.up2 = Up(512 // factor + 256, 256 // factor, bilinear)\n",
        "        # up3: up2_output(128) + skip(128) = 256 input channels\n",
        "        self.up3 = Up(256 // factor + 128, 128 // factor, bilinear)\n",
        "        # up4: up3_output(64) + skip(64) = 128 input channels\n",
        "        self.up4 = Up(128 // factor + 64, 64, bilinear)\n",
        "\n",
        "        # Output layer\n",
        "        self.outc = OutConv(64, n_classes)\n",
        "\n",
        "        # Dropout for regularization\n",
        "        self.dropout = nn.Dropout2d(dropout_rate)\n",
        "\n",
        "        # Initialize weights\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def get_device(self) -> torch.device:\n",
        "        \"\"\"Get the device the model is on.\"\"\"\n",
        "        return self.device\n",
        "\n",
        "    def to_device(self, device: str) -> 'UNet':\n",
        "        \"\"\"Move model to specified device.\"\"\"\n",
        "        self.device = torch.device(device)\n",
        "        return self.to(self.device)\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        \"\"\"Initialize model weights using Xavier/Glorot initialization.\"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass through the UNet.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch_size, channels, height, width)\n",
        "               Expected input size: (B, 1, 240, 240)\n",
        "\n",
        "        Returns:\n",
        "            Output tensor of shape (batch_size, n_classes, height, width)\n",
        "            Output size: (B, 4, 240, 240)\n",
        "        \"\"\"\n",
        "        # Ensure input is on the correct device\n",
        "        if x.device != self.device:\n",
        "            x = x.to(self.device)\n",
        "        # Encoder path with skip connections\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "\n",
        "        # Apply dropout at bottleneck\n",
        "        x5 = self.dropout(x5)\n",
        "        x5 = self.bottleneck(x5)\n",
        "\n",
        "        # Decoder path with skip connections\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "\n",
        "        # Output layer\n",
        "        logits = self.outc(x)\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wMlk1yvH8on"
      },
      "outputs": [],
      "source": [
        "def get_model_summary(model: nn.Module, input_size: Tuple[int, int, int, int] = (1, 1, 240, 240)) -> str:\n",
        "    \"\"\"\n",
        "    Generate a summary of the model architecture.\n",
        "\n",
        "    Args:\n",
        "        model: PyTorch model\n",
        "        input_size: Input tensor size (batch_size, channels, height, width)\n",
        "\n",
        "    Returns:\n",
        "        String containing model summary\n",
        "    \"\"\"\n",
        "    def register_hook(module):\n",
        "        def hook(module, input, output):\n",
        "            class_name = str(module.__class__).split(\".\")[-1].split(\"'\")[0]\n",
        "            module_idx = len(summary)\n",
        "\n",
        "            m_key = f\"{class_name}-{module_idx + 1}\"\n",
        "            summary[m_key] = OrderedDict()\n",
        "            summary[m_key][\"module_name\"] = class_name\n",
        "            summary[m_key][\"input_shape\"] = list(input[0].size())\n",
        "            summary[m_key][\"output_shape\"] = list(output.size())\n",
        "            summary[m_key][\"num_params\"] = sum(p.numel() for p in module.parameters())\n",
        "\n",
        "        hooks.append(module.register_forward_hook(hook))\n",
        "\n",
        "    # Create OrderedDict to store summary\n",
        "    from collections import OrderedDict\n",
        "    summary = OrderedDict()\n",
        "    hooks = []\n",
        "\n",
        "    # Register hook\n",
        "    model.apply(register_hook)\n",
        "\n",
        "    # Make a forward pass\n",
        "    x = torch.zeros(input_size).to(next(model.parameters()).device)\n",
        "    model(x)\n",
        "\n",
        "    # Remove hooks\n",
        "    for h in hooks:\n",
        "        h.remove()\n",
        "\n",
        "    # Generate summary string\n",
        "    summary_str = f\"{'='*80}\\n\"\n",
        "    summary_str += f\"{'UNet Model Summary':^80}\\n\"\n",
        "    summary_str += f\"{'='*80}\\n\"\n",
        "    summary_str += f\"{'Layer (type)':<25} {'Output Shape':<25} {'Param #':<15}\\n\"\n",
        "    summary_str += f\"{'='*80}\\n\"\n",
        "\n",
        "    total_params = 0\n",
        "    for layer in summary:\n",
        "        summary_str += f\"{summary[layer]['module_name']:<25} \"\n",
        "        summary_str += f\"{str(summary[layer]['output_shape']):<25} \"\n",
        "        summary_str += f\"{summary[layer]['num_params']:<15}\\n\"\n",
        "        total_params += summary[layer][\"num_params\"]\n",
        "\n",
        "    summary_str += f\"{'='*80}\\n\"\n",
        "    summary_str += f\"Total params: {total_params:,}\\n\"\n",
        "    summary_str += f\"Trainable params: {total_params:,}\\n\"\n",
        "    summary_str += f\"{'='*80}\\n\"\n",
        "\n",
        "    return summary_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qrQV7eUcZze"
      },
      "outputs": [],
      "source": [
        "class BraTSDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset class for BraTS brain tumor segmentation data.\n",
        "    Loads MRI slices and corresponding segmentation masks.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_dir: str, transform=None, slice_range: Optional[Tuple[int, int]] = None,\n",
        "                 patient_dirs: Optional[List[Path]] = None, modality: str = 't1'):\n",
        "        \"\"\"\n",
        "        Initialize BraTS dataset.\n",
        "\n",
        "        Args:\n",
        "            data_dir: Directory containing BraTS data\n",
        "            transform: Optional transforms to apply\n",
        "            slice_range: Tuple of (start, end) slice indices to use (None for single middle slice)\n",
        "            patient_dirs: Optional list of specific patient directories to use (for train/val/test splits)\n",
        "            modality: MRI modality to use ('t1', 't1ce', 't2', 'flair')\n",
        "        \"\"\"\n",
        "        self.data_dir = Path(data_dir)\n",
        "        self.transform = transform\n",
        "        self.slice_range = slice_range\n",
        "        self.modality = modality.lower()\n",
        "\n",
        "        if patient_dirs is not None:\n",
        "            # Use provided patient directories (for train/val/test splits)\n",
        "            self.patient_dirs = patient_dirs\n",
        "        else:\n",
        "            # Find all patient directories\n",
        "            self.patient_dirs = []\n",
        "            for patient_dir in self.data_dir.iterdir():\n",
        "                if patient_dir.is_dir() and patient_dir.name.startswith('Brats18'):\n",
        "                    self.patient_dirs.append(patient_dir)\n",
        "\n",
        "        print(f\"Found {len(self.patient_dirs)} patient directories\")\n",
        "\n",
        "        # Validate data structure\n",
        "        self._validate_data()\n",
        "\n",
        "    def _validate_data(self):\n",
        "        \"\"\"Validate that required files exist for each patient.\"\"\"\n",
        "        valid_patients = []\n",
        "\n",
        "        for patient_dir in self.patient_dirs:\n",
        "            # Check for required files\n",
        "            modality_file = patient_dir / f\"{patient_dir.name}_{self.modality}.nii\"\n",
        "            seg_file = patient_dir / f\"{patient_dir.name}_seg.nii\"\n",
        "\n",
        "            if modality_file.exists() and seg_file.exists():\n",
        "                valid_patients.append(patient_dir)\n",
        "            else:\n",
        "                print(f\"Warning: Missing {self.modality} or seg files for {patient_dir.name}\")\n",
        "\n",
        "        self.patient_dirs = valid_patients\n",
        "        print(f\"Valid patients with {self.modality.upper()} modality: {len(self.patient_dirs)}\")\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        if self.slice_range is None:\n",
        "            return len(self.patient_dirs)\n",
        "        else:\n",
        "            # Each patient contributes (end - start) slices\n",
        "            return len(self.patient_dirs) * (self.slice_range[1] - self.slice_range[0])\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Get MRI slice and segmentation mask for a patient.\"\"\"\n",
        "        if self.slice_range is None:\n",
        "            # Single slice mode (original behavior)\n",
        "            patient_dir = self.patient_dirs[idx]\n",
        "            patient_idx = idx\n",
        "            slice_idx = None\n",
        "        else:\n",
        "            # Multiple slices mode\n",
        "            slices_per_patient = self.slice_range[1] - self.slice_range[0]\n",
        "            patient_idx = idx // slices_per_patient\n",
        "            slice_offset = idx % slices_per_patient\n",
        "            slice_idx = self.slice_range[0] + slice_offset\n",
        "            patient_dir = self.patient_dirs[patient_idx]\n",
        "\n",
        "        # Load modality image\n",
        "        modality_file = patient_dir / f\"{patient_dir.name}_{self.modality}.nii\"\n",
        "        modality_img = nib.load(str(modality_file))\n",
        "        modality_data = modality_img.get_fdata()\n",
        "\n",
        "        # Load segmentation mask\n",
        "        seg_file = patient_dir / f\"{patient_dir.name}_seg.nii\"\n",
        "        seg_img = nib.load(str(seg_file))\n",
        "        seg_data = seg_img.get_fdata()\n",
        "\n",
        "        # Determine slice index\n",
        "        if slice_idx is None:\n",
        "            # Use middle slice for single slice mode\n",
        "            slice_idx = modality_data.shape[2] // 2\n",
        "        else:\n",
        "            # Ensure slice index is within bounds\n",
        "            slice_idx = min(slice_idx, modality_data.shape[2] - 1)\n",
        "\n",
        "        # Extract slice\n",
        "        modality_slice = modality_data[:, :, slice_idx]\n",
        "        seg_slice = seg_data[:, :, slice_idx]\n",
        "\n",
        "        # Normalize modality data to [0, 1]\n",
        "        modality_slice = (modality_slice - modality_slice.min()) / (modality_slice.max() - modality_slice.min() + 1e-8)\n",
        "\n",
        "        # Convert segmentation to multi-class format\n",
        "        # BraTS: 0=background, 1=necrotic, 2=edema, 4=enhancing\n",
        "        # Keep original BraTS labels [0,1,2,4]\n",
        "        seg_slice = seg_slice.astype(np.uint8)\n",
        "\n",
        "        # Debug label information (only for first few samples)\n",
        "        if hasattr(self, '_debug_count'):\n",
        "            self._debug_count += 1\n",
        "        else:\n",
        "            self._debug_count = 1\n",
        "\n",
        "        if self._debug_count <= 3:  # Only debug first 3 samples\n",
        "            unique_labels = np.unique(seg_slice)\n",
        "            print(f\"Sample {self._debug_count} - Patient: {patient_dir.name}, Slice: {slice_idx}\")\n",
        "            print(f\"Sample {self._debug_count} - BraTS labels: {unique_labels}\")\n",
        "            print(f\"Sample {self._debug_count} - Label range: [{seg_slice.min()}, {seg_slice.max()}]\")\n",
        "\n",
        "        # Convert to tensor format\n",
        "        modality_tensor = torch.FloatTensor(modality_slice).unsqueeze(0)  # (1, H, W)\n",
        "        seg_tensor = torch.LongTensor(seg_slice)  # (H, W)\n",
        "\n",
        "        # Apply transforms if any\n",
        "        if self.transform:\n",
        "            modality_tensor, seg_tensor = self.transform(modality_tensor, seg_tensor)\n",
        "\n",
        "        return modality_tensor, seg_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUC8-rwq7dC5"
      },
      "outputs": [],
      "source": [
        "def create_patient_splits(data_dir: str, train_ratio: float = 0.7, val_ratio: float = 0.15,\n",
        "                         test_ratio: float = 0.15, random_seed: int = 42, modality: str = 't1') -> Tuple[List[Path], List[Path], List[Path]]:\n",
        "    \"\"\"\n",
        "    Create patient-level train/validation/test splits.\n",
        "\n",
        "    Args:\n",
        "        data_dir: Directory containing BraTS data\n",
        "        train_ratio: Ratio of patients for training\n",
        "        val_ratio: Ratio of patients for validation\n",
        "        test_ratio: Ratio of patients for testing\n",
        "        random_seed: Random seed for reproducible splits\n",
        "        modality: MRI modality to use ('t1', 't1ce', 't2', 'flair')\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (train_patients, val_patients, test_patients)\n",
        "    \"\"\"\n",
        "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, \"Ratios must sum to 1.0\"\n",
        "\n",
        "    # Set random seed for reproducibility\n",
        "    np.random.seed(random_seed)\n",
        "\n",
        "    # Find all patient directories\n",
        "    data_path = Path(data_dir)\n",
        "    all_patients = []\n",
        "\n",
        "    for patient_dir in data_path.iterdir():\n",
        "        if patient_dir.is_dir() and patient_dir.name.startswith('Brats18'):\n",
        "            # Check if required files exist\n",
        "            modality_file = patient_dir / f\"{patient_dir.name}_{modality}.nii\"\n",
        "            seg_file = patient_dir / f\"{patient_dir.name}_seg.nii\"\n",
        "            if modality_file.exists() and seg_file.exists():\n",
        "                all_patients.append(patient_dir)\n",
        "\n",
        "    print(f\"Found {len(all_patients)} valid patients\")\n",
        "\n",
        "    # Shuffle patients\n",
        "    np.random.shuffle(all_patients)\n",
        "\n",
        "    # Calculate split indices\n",
        "    n_patients = len(all_patients)\n",
        "    n_train = int(n_patients * train_ratio)\n",
        "    n_val = int(n_patients * val_ratio)\n",
        "    n_test = n_patients - n_train - n_val  # Ensure all patients are used\n",
        "\n",
        "    # Split patients\n",
        "    train_patients = all_patients[:n_train]\n",
        "    val_patients = all_patients[n_train:n_train + n_val]\n",
        "    test_patients = all_patients[n_train + n_val:]\n",
        "\n",
        "    print(f\"Patient splits:\")\n",
        "    print(f\"  Training: {len(train_patients)} patients ({len(train_patients)/n_patients:.1%})\")\n",
        "    print(f\"  Validation: {len(val_patients)} patients ({len(val_patients)/n_patients:.1%})\")\n",
        "    print(f\"  Test: {len(test_patients)} patients ({len(test_patients)/n_patients:.1%})\")\n",
        "\n",
        "    return train_patients, val_patients, test_patients\n",
        "\n",
        "\n",
        "def create_datasets_with_splits(data_dir: str, slice_range: Optional[Tuple[int, int]] = None,\n",
        "                               train_ratio: float = 0.7, val_ratio: float = 0.15, test_ratio: float = 0.15,\n",
        "                               random_seed: int = 42, modality: str = 't1') -> Tuple[BraTSDataset, BraTSDataset, BraTSDataset]:\n",
        "    \"\"\"\n",
        "    Create train/validation/test datasets with patient-level splits.\n",
        "\n",
        "    Args:\n",
        "        data_dir: Directory containing BraTS data\n",
        "        slice_range: Tuple of (start, end) slice indices to use\n",
        "        train_ratio: Ratio of patients for training\n",
        "        val_ratio: Ratio of patients for validation\n",
        "        test_ratio: Ratio of patients for testing\n",
        "        random_seed: Random seed for reproducible splits\n",
        "        modality: MRI modality to use ('t1', 't1ce', 't2', 'flair')\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (train_dataset, val_dataset, test_dataset)\n",
        "    \"\"\"\n",
        "    # Create patient splits\n",
        "    train_patients, val_patients, test_patients = create_patient_splits(\n",
        "        data_dir, train_ratio, val_ratio, test_ratio, random_seed, modality\n",
        "    )\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = BraTSDataset(\n",
        "        data_dir=data_dir,\n",
        "        slice_range=slice_range,\n",
        "        patient_dirs=train_patients,\n",
        "        modality=modality\n",
        "    )\n",
        "\n",
        "    val_dataset = BraTSDataset(\n",
        "        data_dir=data_dir,\n",
        "        slice_range=slice_range,\n",
        "        patient_dirs=val_patients,\n",
        "        modality=modality\n",
        "    )\n",
        "\n",
        "    test_dataset = BraTSDataset(\n",
        "        data_dir=data_dir,\n",
        "        slice_range=slice_range,\n",
        "        patient_dirs=test_patients,\n",
        "        modality=modality\n",
        "    )\n",
        "\n",
        "    return train_dataset, val_dataset, test_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FA3wD-uscmUn"
      },
      "outputs": [],
      "source": [
        "# Create dataloaders\n",
        "train_dataset, val_dataset, test_dataset = create_datasets_with_splits(\n",
        "        data_dir='/content/MICCAI_BraTS_2018_Data_Training/training/HGG',\n",
        "        slice_range = (30, 121)\n",
        "    )\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Validation samples: {len(val_dataset)}\")\n",
        "print(f\"Testing samples: {len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQJqlmvb_vsw"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=10,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=10,\n",
        "        shuffle=False,\n",
        "        num_workers=0\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=10,\n",
        "        shuffle=False,\n",
        "        num_workers=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "co6SsP9Q8EYz"
      },
      "outputs": [],
      "source": [
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Validation samples: {len(val_dataset)}\")\n",
        "print(f\"Testing samples: {len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3coRvHqlLmaj"
      },
      "outputs": [],
      "source": [
        "def move_batch_to_device(batch, device):\n",
        "    \"\"\"Move a batch of tensors to the specified device.\"\"\"\n",
        "    if isinstance(batch, (list, tuple)):\n",
        "        return [tensor.to(device) for tensor in batch]\n",
        "    else:\n",
        "        return batch.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umXz70Z39ebm"
      },
      "outputs": [],
      "source": [
        "def calculate_iou(pred_mask: np.ndarray, gt_mask: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Calculate Intersection over Union (IoU) between predicted and ground truth masks.\n",
        "\n",
        "    Args:\n",
        "        pred_mask: Predicted segmentation mask (binary)\n",
        "        gt_mask: Ground truth segmentation mask (binary)\n",
        "\n",
        "    Returns:\n",
        "        IoU score as float\n",
        "    \"\"\"\n",
        "    # Ensure masks are binary\n",
        "    pred_binary = (pred_mask > 0).astype(np.uint8)\n",
        "    gt_binary = (gt_mask > 0).astype(np.uint8)\n",
        "\n",
        "    # Calculate intersection and union\n",
        "    intersection = np.logical_and(pred_binary, gt_binary).sum()\n",
        "    union = np.logical_or(pred_binary, gt_binary).sum()\n",
        "\n",
        "    # Avoid division by zero\n",
        "    if union == 0:\n",
        "        return 1.0 if intersection == 0 else 0.0\n",
        "\n",
        "    return intersection / union\n",
        "\n",
        "\n",
        "def calculate_dice_score(pred_mask: np.ndarray, gt_mask: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Calculate Dice score between predicted and ground truth masks.\n",
        "\n",
        "    Args:\n",
        "        pred_mask: Predicted segmentation mask (binary)\n",
        "        gt_mask: Ground truth segmentation mask (binary)\n",
        "\n",
        "    Returns:\n",
        "        Dice score as float\n",
        "    \"\"\"\n",
        "    # Ensure masks are binary\n",
        "    pred_binary = (pred_mask > 0).astype(np.uint8)\n",
        "    gt_binary = (gt_mask > 0).astype(np.uint8)\n",
        "\n",
        "    # Calculate intersection and sum\n",
        "    intersection = np.logical_and(pred_binary, gt_binary).sum()\n",
        "    sum_masks = pred_binary.sum() + gt_binary.sum()\n",
        "\n",
        "    # Avoid division by zero\n",
        "    if sum_masks == 0:\n",
        "        return 1.0 if intersection == 0 else 0.0\n",
        "\n",
        "    return (2 * intersection) / sum_masks\n",
        "\n",
        "def calculate_segmentation_metrics(pred_mask: np.ndarray, gt_mask: np.ndarray) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Calculate comprehensive segmentation metrics.\n",
        "\n",
        "    Args:\n",
        "        pred_mask: Predicted segmentation mask\n",
        "        gt_mask: Ground truth segmentation mask\n",
        "\n",
        "    Returns:\n",
        "        Dictionary containing various metrics\n",
        "    \"\"\"\n",
        "    # Ensure masks are binary\n",
        "    pred_binary = (pred_mask > 0).astype(np.uint8)\n",
        "    gt_binary = (gt_mask > 0).astype(np.uint8)\n",
        "\n",
        "    # Calculate basic metrics\n",
        "    intersection = np.logical_and(pred_binary, gt_binary).sum()\n",
        "    union = np.logical_or(pred_binary, gt_binary).sum()\n",
        "    pred_volume = pred_binary.sum()\n",
        "    gt_volume = gt_binary.sum()\n",
        "\n",
        "    # Calculate IoU and Dice\n",
        "    iou = calculate_iou(pred_binary, gt_binary)\n",
        "    dice = calculate_dice_score(pred_binary, gt_binary)\n",
        "\n",
        "    # Calculate precision and recall\n",
        "    if pred_volume > 0:\n",
        "        precision = intersection / pred_volume\n",
        "    else:\n",
        "        precision = 1.0 if gt_volume == 0 else 0.0\n",
        "\n",
        "    if gt_volume > 0:\n",
        "        recall = intersection / gt_volume\n",
        "    else:\n",
        "        recall = 1.0 if pred_volume == 0 else 0.0\n",
        "\n",
        "    # Calculate F1 score (same as Dice score)\n",
        "    f1_score = dice\n",
        "\n",
        "    # Calculate volume similarity\n",
        "    if pred_volume + gt_volume > 0:\n",
        "        volume_similarity = 1 - abs(pred_volume - gt_volume) / (pred_volume + gt_volume)\n",
        "    else:\n",
        "        volume_similarity = 1.0\n",
        "\n",
        "    # Calculate relative volume difference\n",
        "    if gt_volume > 0:\n",
        "        relative_volume_difference = abs(pred_volume - gt_volume) / gt_volume\n",
        "    else:\n",
        "        relative_volume_difference = 0.0 if pred_volume == 0 else float('inf')\n",
        "\n",
        "    metrics = {\n",
        "        'iou': iou,\n",
        "        'dice_score': dice,\n",
        "        'f1_score': f1_score,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'volume_similarity': volume_similarity,\n",
        "        'relative_volume_difference': relative_volume_difference,\n",
        "        'intersection': int(intersection),\n",
        "        'union': int(union),\n",
        "        'predicted_volume': int(pred_volume),\n",
        "        'ground_truth_volume': int(gt_volume)\n",
        "    }\n",
        "\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6T-QwN0Q33j"
      },
      "outputs": [],
      "source": [
        "class BraTSCrossEntropyLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    CrossEntropyLoss adapted for BraTS non-consecutive labels [0,1,2,4].\n",
        "    Maps BraTS labels directly to model output indices.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(BraTSCrossEntropyLoss, self).__init__()\n",
        "        # BraTS labels: [0,1,2,4] -> model output indices: [0,1,2,3]\n",
        "        # Direct mapping: 0->0, 1->1, 2->2, 4->3\n",
        "        self.label_mapping = {0: 0, 1: 1, 2: 2, 4: 3}\n",
        "        self.ce_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, inputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass with BraTS label mapping.\n",
        "\n",
        "        Args:\n",
        "            inputs: Model outputs (B, 4, H, W) - 4 classes for [0,1,2,4]\n",
        "            targets: Ground truth labels (B, H, W) with values [0,1,2,4]\n",
        "\n",
        "        Returns:\n",
        "            CrossEntropyLoss value\n",
        "        \"\"\"\n",
        "        # Map BraTS labels to model output indices\n",
        "        mapped_targets = targets.clone()\n",
        "        for brats_label, consecutive_idx in self.label_mapping.items():\n",
        "            mapped_targets[targets == brats_label] = consecutive_idx\n",
        "\n",
        "        # Use all 4 output channels (corresponding to BraTS labels [0,1,2,4])\n",
        "        return self.ce_loss(inputs, mapped_targets)\n",
        "\n",
        "        return self.ce_loss(valid_outputs, mapped_targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBWETU0qYkzC"
      },
      "outputs": [],
      "source": [
        "class BraTSDiceLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Dice Loss adapted for BraTS non-consecutive labels [0,1,2,4].\n",
        "    Maps BraTS labels to consecutive indices for Dice computation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, smooth: float = 1e-6):\n",
        "        super(BraTSDiceLoss, self).__init__()\n",
        "        self.smooth = smooth\n",
        "        # BraTS labels: [0,1,2,4] -> consecutive indices: [0,1,2,3]\n",
        "        self.label_mapping = {0: 0, 1: 1, 2: 2, 4: 3}\n",
        "        self.reverse_mapping = {0: 0, 1: 1, 2: 2, 3: 4}\n",
        "\n",
        "    def forward(self, inputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            inputs: Model predictions (N, 4, H, W) - raw logits for 4 classes\n",
        "            targets: Ground truth labels (N, H, W) - BraTS labels [0,1,2,4]\n",
        "\n",
        "        Returns:\n",
        "            Dice loss (1 - mean Dice coefficient)\n",
        "        \"\"\"\n",
        "        # Convert logits to probabilities\n",
        "        probs = torch.softmax(inputs, dim=1)\n",
        "\n",
        "        # Map BraTS targets to consecutive indices\n",
        "        targets_mapped = torch.zeros_like(targets)\n",
        "        for brats_label, consecutive_idx in self.label_mapping.items():\n",
        "            mask = (targets == brats_label)\n",
        "            targets_mapped[mask] = consecutive_idx\n",
        "\n",
        "        # Convert to one-hot encoding\n",
        "        targets_one_hot = torch.zeros_like(probs)\n",
        "        targets_one_hot.scatter_(1, targets_mapped.unsqueeze(1), 1)\n",
        "\n",
        "        # Compute Dice coefficient for each class\n",
        "        dice_scores = []\n",
        "        for c in range(4):  # 4 classes: [0,1,2,3] corresponding to BraTS [0,1,2,4]\n",
        "            pred_c = probs[:, c, :, :]\n",
        "            target_c = targets_one_hot[:, c, :, :]\n",
        "\n",
        "            # Compute intersection and union\n",
        "            intersection = (pred_c * target_c).sum(dim=(1, 2))  # (N,)\n",
        "            union = pred_c.sum(dim=(1, 2)) + target_c.sum(dim=(1, 2))  # (N,)\n",
        "\n",
        "            # Compute Dice coefficient for this class\n",
        "            dice_c = (2.0 * intersection + self.smooth) / (union + self.smooth)\n",
        "            dice_scores.append(dice_c)\n",
        "\n",
        "        # Stack all class Dice scores: (4, N)\n",
        "        dice_scores = torch.stack(dice_scores, dim=0)\n",
        "\n",
        "        # Compute mean Dice across classes and batch\n",
        "        mean_dice = dice_scores.mean()\n",
        "\n",
        "        # Return Dice loss (1 - Dice)\n",
        "        return 1.0 - mean_dice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrtTqqYN8Iu3"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model: nn.Module,\n",
        "                dataloader: DataLoader,\n",
        "                criterion: nn.Module,\n",
        "                optimizer: optim.Optimizer) -> Tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Train for one epoch.\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (average loss, average accuracy)\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_pixels = 0\n",
        "\n",
        "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
        "\n",
        "    for batch_idx, (images, targets) in enumerate(progress_bar):\n",
        "        # Move tensors to model's device\n",
        "        images, targets = move_batch_to_device((images, targets), model.get_device())\n",
        "\n",
        "        # Debug device information (only on first batch)\n",
        "        if batch_idx == 0:\n",
        "            print(f\"Debug - Images device: {images.device}, Targets device: {targets.device}\")\n",
        "            print(f\"Debug - Model device: {model.get_device()}\")\n",
        "            print(f\"Debug - Images shape: {images.shape}, Targets shape: {targets.shape}\")\n",
        "            print(f\"Debug - Images dtype: {images.dtype}, Targets dtype: {targets.dtype}\")\n",
        "            print(f\"Debug - Images range: [{images.min():.3f}, {images.max():.3f}]\")\n",
        "            print(f\"Debug - Targets range: [{targets.min()}, {targets.max()}]\")\n",
        "            print(f\"Debug - Unique target values: {torch.unique(targets)}\")\n",
        "            print(f\"Debug - Number of classes expected: {model.n_classes}\")\n",
        "\n",
        "        # Validate target values for BraTS labels [0,1,2,4]\n",
        "        valid_brats_labels = [0, 1, 2, 4]\n",
        "        unique_targets = torch.unique(targets).cpu().numpy()\n",
        "        invalid_labels = [label for label in unique_targets if label not in valid_brats_labels]\n",
        "\n",
        "        if invalid_labels:\n",
        "            print(f\"ERROR: Invalid BraTS target values! Found: {invalid_labels}, Expected: {valid_brats_labels}\")\n",
        "            raise ValueError(f\"Target values must be in BraTS format: {valid_brats_labels}\")\n",
        "\n",
        "        # Check for NaN or Inf values\n",
        "        if torch.isnan(images).any():\n",
        "            print(\"ERROR: NaN values found in images!\")\n",
        "            raise ValueError(\"Images contain NaN values\")\n",
        "        if torch.isnan(targets).any():\n",
        "            print(\"ERROR: NaN values found in targets!\")\n",
        "            raise ValueError(\"Targets contain NaN values\")\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        try:\n",
        "            outputs = model(images)\n",
        "\n",
        "            # Debug output device\n",
        "            if batch_idx == 0:\n",
        "                print(f\"Debug - Outputs device: {outputs.device}\")\n",
        "\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            # Debug loss device\n",
        "            if batch_idx == 0:\n",
        "                print(f\"Debug - Loss device: {loss.device}\")\n",
        "\n",
        "        except RuntimeError as e:\n",
        "            if \"device\" in str(e).lower():\n",
        "                print(f\"Device error in forward pass: {e}\")\n",
        "                print(f\"Images device: {images.device}\")\n",
        "                print(f\"Targets device: {targets.device}\")\n",
        "                print(f\"Model device: {model.get_device()}\")\n",
        "                raise\n",
        "            else:\n",
        "                raise\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Calculate accuracy with BraTS label mapping\n",
        "        # Map model outputs back to BraTS labels for comparison\n",
        "        pred_model_indices = torch.argmax(outputs, dim=1)  # Use all 4 channels\n",
        "        pred_brats = pred_model_indices.clone()\n",
        "\n",
        "\n",
        "        # Map model indices back to BraTS labels\n",
        "        brats_mapping = {0: 0, 1: 1, 2: 2, 3: 4}  # model index -> BraTS label\n",
        "        for model_idx, brats_label in brats_mapping.items():\n",
        "            pred_brats[pred_model_indices == model_idx] = brats_label\n",
        "\n",
        "        correct = (pred_brats == targets).sum().item()\n",
        "        total_correct += correct\n",
        "        total_pixels += targets.numel()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Update progress bar\n",
        "        progress_bar.set_postfix({\n",
        "            'Loss': f'{loss.item():.4f}',\n",
        "            'Acc': f'{100 * correct / targets.numel():.2f}%'\n",
        "        })\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    avg_accuracy = total_correct / total_pixels\n",
        "\n",
        "    return avg_loss, avg_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6o56no38o_-"
      },
      "outputs": [],
      "source": [
        "def validate_epoch(model: nn.Module,\n",
        "                   dataloader: DataLoader,\n",
        "                   criterion: nn.Module) -> Tuple[float, float, dict]:\n",
        "    \"\"\"\n",
        "    Validate for one epoch.\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (average loss, average accuracy, metrics dict)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_pixels = 0\n",
        "\n",
        "    all_metrics = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (images, targets) in enumerate(tqdm(dataloader, desc=\"Validation\")):\n",
        "            # Move tensors to model's device\n",
        "            images, targets = move_batch_to_device((images, targets), model.get_device())\n",
        "\n",
        "            # Debug device information (only on first batch)\n",
        "            if batch_idx == 0:\n",
        "                print(f\"Val Debug - Images device: {images.device}, Targets device: {targets.device}\")\n",
        "                print(f\"Val Debug - Model device: {model.get_device()}\")\n",
        "\n",
        "            # Forward pass\n",
        "            try:\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, targets)\n",
        "            except RuntimeError as e:\n",
        "                if \"device\" in str(e).lower():\n",
        "                    print(f\"Device error in validation: {e}\")\n",
        "                    print(f\"Images device: {images.device}\")\n",
        "                    print(f\"Targets device: {targets.device}\")\n",
        "                    print(f\"Model device: {model.get_device()}\")\n",
        "                    raise\n",
        "                else:\n",
        "                    raise\n",
        "\n",
        "            # Calculate accuracy with BraTS label mapping\n",
        "            pred_model_indices = torch.argmax(outputs, dim=1)  # Use all 4 channels\n",
        "            pred_brats = pred_model_indices.clone()\n",
        "\n",
        "            # Map model indices back to BraTS labels\n",
        "            brats_mapping = {0: 0, 1: 1, 2: 2, 3: 4}  # model index -> BraTS label\n",
        "            for model_idx, brats_label in brats_mapping.items():\n",
        "                pred_brats[pred_model_indices == model_idx] = brats_label\n",
        "\n",
        "            correct = (pred_brats == targets).sum().item()\n",
        "            total_correct += correct\n",
        "            total_pixels += targets.numel()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Calculate metrics for each sample in batch\n",
        "            for i in range(images.size(0)):\n",
        "                pred_mask = pred_brats[i].cpu().numpy()\n",
        "                gt_mask = targets[i].cpu().numpy()\n",
        "\n",
        "                # Convert to binary for metrics (any non-zero class)\n",
        "                pred_binary = (pred_mask > 0).astype(np.uint8)\n",
        "                gt_binary = (gt_mask > 0).astype(np.uint8)\n",
        "\n",
        "                metrics = calculate_segmentation_metrics(pred_binary, gt_binary)\n",
        "                all_metrics.append(metrics)\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    avg_accuracy = total_correct / total_pixels\n",
        "\n",
        "    # Calculate average metrics\n",
        "    avg_metrics = {}\n",
        "    if all_metrics:\n",
        "        for key in all_metrics[0].keys():\n",
        "            if isinstance(all_metrics[0][key], (int, float)):\n",
        "                values = [m[key] for m in all_metrics if key in m]\n",
        "                avg_metrics[f'avg_{key}'] = np.mean(values)\n",
        "                avg_metrics[f'std_{key}'] = np.std(values)\n",
        "\n",
        "    return avg_loss, avg_accuracy, avg_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_OnvcR699CC"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(model: nn.Module,\n",
        "                   optimizer: optim.Optimizer,\n",
        "                   epoch: int,\n",
        "                   loss: float,\n",
        "                   save_path: str):\n",
        "    \"\"\"Save model checkpoint.\"\"\"\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': loss,\n",
        "        'timestamp': datetime.now().isoformat()\n",
        "    }\n",
        "\n",
        "    torch.save(checkpoint, save_path)\n",
        "    print(f\"Checkpoint saved to {save_path}\")\n",
        "\n",
        "\n",
        "def load_checkpoint(model: nn.Module,\n",
        "                   optimizer: optim.Optimizer,\n",
        "                   checkpoint_path: str) -> int:\n",
        "    \"\"\"Load model checkpoint and return starting epoch.\"\"\"\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        start_epoch = checkpoint['epoch'] + 1\n",
        "        print(f\"Loaded checkpoint from epoch {checkpoint['epoch']}\")\n",
        "        return start_epoch\n",
        "    else:\n",
        "        print(\"No checkpoint found, starting from epoch 0\")\n",
        "        return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFtRkCteMtTM"
      },
      "outputs": [],
      "source": [
        "!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJO8XeNQHkeF"
      },
      "outputs": [],
      "source": [
        "# # Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Create model\n",
        "model = UNet(n_classes=4)\n",
        "\n",
        "# Move model to device\n",
        "model = model.to(device=device)\n",
        "\n",
        "# Print model summary\n",
        "print(get_model_summary(model))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0zRBSRI-9PA"
      },
      "outputs": [],
      "source": [
        "from time import strftime\n",
        "# dataset details\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Validation samples: {len(val_dataset)}\")\n",
        "print(f\"model on device: {model.get_device()}\")\n",
        "\n",
        "# Loss function and optimizer\n",
        "\n",
        "criterions = [BraTSDiceLoss(), BraTSCrossEntropyLoss()]\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
        "save_dirs = []\n",
        "\n",
        "for criterion in criterions:\n",
        "    print(f\"Using {criterion.__class__.__name__} for BraTS labels [0,1,2,4]\")\n",
        "    save_dir = f\"/content/gdrive/MyDrive/dhai/checkpoints/2025-09-05/{criterion.__class__.__name__}\"\n",
        "    print(f\"save directory = {save_dir}\")\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    save_dirs.append(save_dir)\n",
        "\n",
        "    # Load checkpoint if exists\n",
        "    start_epoch = load_checkpoint(model, optimizer, os.path.join(save_dir, 'unet_brats_t1.pth'))\n",
        "    num_epochs = 10\n",
        "\n",
        "    # Training history\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "    loss_method_to_train_losses = {}\n",
        "    loss_method_to_val_lossews = {}\n",
        "\n",
        "    # Training loop\n",
        "    print(f\"Starting training from epoch {start_epoch+1}\")\n",
        "\n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Train\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer)\n",
        "\n",
        "        # Validate\n",
        "        val_loss, val_acc, val_metrics = validate_epoch(model, val_loader, criterion)\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # Store history\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        val_accs.append(val_acc)\n",
        "\n",
        "        # Print results\n",
        "        print(f\"Training Loss: {train_loss:.4f}, Accuracy: {100*train_acc:.2f}%\")\n",
        "        print(f\"Validation Loss: {val_loss:.4f}, Accuracy: {100*val_acc:.2f}%\")\n",
        "\n",
        "        if val_metrics:\n",
        "            print(f\"Validation IoU: {val_metrics.get('avg_iou', 0):.4f}\")\n",
        "            print(f\"Validation Dice: {val_metrics.get('avg_dice_score', 0):.4f}\")\n",
        "\n",
        "        # Save checkpoint\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            checkpoint_path = os.path.join(\n",
        "                save_dir,\n",
        "                f'unet_brats_t1_epoch_{epoch+1}.pth'\n",
        "            )\n",
        "            save_checkpoint(model, optimizer, epoch, val_loss, checkpoint_path)\n",
        "\n",
        "    # Save final model\n",
        "    final_checkpoint_path = os.path.join(save_dir, 'unet_brats_final.pth')\n",
        "    save_checkpoint(model, optimizer, num_epochs-1, val_loss, final_checkpoint_path)\n",
        "    loss_method_to_train_losses[criterion.__class__.__name__] = train_losses\n",
        "    loss_method_to_val_lossews[criterion.__class__.__name__] = val_losses\n",
        "\n",
        "print(\"Training completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOKsGyZjeDMS"
      },
      "outputs": [],
      "source": [
        "pred_masks = []\n",
        "gnd_truth_masks = []\n",
        "img_files = []\n",
        "\n",
        "class UNetInference:\n",
        "    \"\"\"\n",
        "    Class for performing inference with trained UNet models.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_path: str, device: str = \"auto\"):\n",
        "        \"\"\"\n",
        "        Initialize UNet inference.\n",
        "\n",
        "        Args:\n",
        "            model_path: Path to trained model checkpoint\n",
        "            device: Device to use (\"auto\", \"cpu\", or \"cuda\")\n",
        "        \"\"\"\n",
        "        self.model_path = model_path\n",
        "\n",
        "        # Set device\n",
        "        if device == \"auto\":\n",
        "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        else:\n",
        "            self.device = torch.device(device)\n",
        "\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "        # Load model\n",
        "        self.model = self._load_model()\n",
        "        self.model.eval()\n",
        "\n",
        "    def _load_model(self) -> torch.nn.Module:\n",
        "        \"\"\"Load the trained UNet model.\"\"\"\n",
        "        # Create model architecture\n",
        "        model = UNet(n_classes=4)\n",
        "\n",
        "        # Load trained weights\n",
        "        if os.path.exists(self.model_path):\n",
        "            checkpoint = torch.load(self.model_path, map_location=self.device)\n",
        "\n",
        "            if 'model_state_dict' in checkpoint:\n",
        "                model.load_state_dict(checkpoint['model_state_dict'])\n",
        "                print(f\"Loaded model from epoch {checkpoint.get('epoch', 'unknown')}\")\n",
        "            else:\n",
        "                # Direct state dict\n",
        "                model.load_state_dict(checkpoint)\n",
        "                print(\"Loaded model state dict directly\")\n",
        "        else:\n",
        "            raise FileNotFoundError(f\"Model checkpoint not found: {self.model_path}\")\n",
        "\n",
        "        model = model.to(self.device)\n",
        "        return model\n",
        "\n",
        "    def batch_inference(self, dataset, output_dir: str,\n",
        "                       save_predictions: bool = True,\n",
        "                       evaluate_predictions: bool = True) -> List[dict]:\n",
        "        \"\"\"\n",
        "        Perform batch inference on a BraTSDataset with 3D volume reconstruction.\n",
        "\n",
        "        Args:\n",
        "            dataset: BraTSDataset instance containing data for inference\n",
        "            output_dir: Directory to save predictions\n",
        "            save_predictions: Whether to save prediction files\n",
        "            evaluate_predictions: Whether to evaluate against ground truth\n",
        "\n",
        "        Returns:\n",
        "            List of evaluation metrics for each patient\n",
        "        \"\"\"\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        print(f\"Processing {len(dataset.patient_dirs)} patients from BraTSDataset\")\n",
        "\n",
        "        all_metrics = []\n",
        "\n",
        "        for patient_idx, patient_dir in enumerate(tqdm(dataset.patient_dirs, desc=\"Patient Inference\")):\n",
        "            try:\n",
        "                # Load the complete 3D volume for this patient\n",
        "                modality_file = patient_dir / f\"{patient_dir.name}_{dataset.modality}.nii\"\n",
        "                seg_file = patient_dir / f\"{patient_dir.name}_seg.nii\"\n",
        "\n",
        "                img_files.append(modality_file)\n",
        "                gnd_truth_masks.append(seg_file)\n",
        "\n",
        "                # Load 3D volumes\n",
        "                modality_img = nib.load(str(modality_file))\n",
        "                modality_data = modality_img.get_fdata()\n",
        "                seg_img = nib.load(str(seg_file))\n",
        "                seg_data = seg_img.get_fdata()\n",
        "\n",
        "                # Get volume dimensions\n",
        "                H, W, D = modality_data.shape\n",
        "\n",
        "                # Initialize 3D prediction volume\n",
        "                prediction_3d = np.zeros((H, W, D), dtype=np.uint8)\n",
        "\n",
        "                # Process each slice\n",
        "                slice_metrics = []\n",
        "\n",
        "                for slice_idx in range(D):\n",
        "                    # Extract slice\n",
        "                    modality_slice = modality_data[:, :, slice_idx]\n",
        "                    seg_slice = seg_data[:, :, slice_idx]\n",
        "\n",
        "                    # Normalize modality data\n",
        "                    modality_slice = (modality_slice - modality_slice.min()) / (modality_slice.max() - modality_slice.min() + 1e-8)\n",
        "\n",
        "                    # Convert to tensor\n",
        "                    modality_tensor = torch.FloatTensor(modality_slice).unsqueeze(0).unsqueeze(0).to(self.device)  # (1, 1, H, W)\n",
        "\n",
        "                    # Perform inference\n",
        "                    with torch.no_grad():\n",
        "                        logits = self.model(modality_tensor)\n",
        "                        predicted_mask = torch.argmax(logits, dim=1)\n",
        "\n",
        "                    # Convert to numpy\n",
        "                    prediction_slice = predicted_mask.cpu().long().numpy().squeeze()  # (H, W)\n",
        "\n",
        "                    # Store in 3D volume\n",
        "                    prediction_3d[:, :, slice_idx] = prediction_slice\n",
        "\n",
        "                    # Calculate slice-level metrics if evaluation is requested\n",
        "                    if evaluate_predictions:\n",
        "                        pred_brats = self._convert_to_brats_labels(prediction_slice)\n",
        "                        gt_brats = self._convert_to_brats_labels(seg_slice.astype(np.uint8))\n",
        "\n",
        "                        # Calculate metrics for this slice\n",
        "                        slice_metric = self._calculate_detailed_metrics(pred_brats, gt_brats)\n",
        "                        slice_metric['slice_idx'] = slice_idx\n",
        "                        slice_metrics.append(slice_metric)\n",
        "\n",
        "                # Convert 3D prediction to BraTS labels\n",
        "                prediction_3d_brats = self._convert_to_brats_labels(prediction_3d)\n",
        "\n",
        "                # Save 3D prediction volume\n",
        "                if save_predictions:\n",
        "                    output_name = f\"pred_{patient_dir.name}.nii.gz\"\n",
        "                    output_path = os.path.join(output_dir, output_name)\n",
        "\n",
        "                    # Create NIfTI image with original affine\n",
        "                    pred_nii = nib.Nifti1Image(prediction_3d_brats, modality_img.affine, dtype=np.uint8)\n",
        "                    nib.save(pred_nii, output_path)\n",
        "                    print(f\"Saved 3D prediction: {output_path}\")\n",
        "                    pred_masks.append(output_path)\n",
        "\n",
        "                # Aggregate slice metrics to patient-level metrics\n",
        "                if evaluate_predictions and slice_metrics:\n",
        "                    patient_metrics = self._aggregate_slice_metrics(slice_metrics, patient_dir.name)\n",
        "                    all_metrics.append(patient_metrics)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing patient {patient_dir.name}: {e}\")\n",
        "                continue\n",
        "\n",
        "        if evaluate_predictions and all_metrics:\n",
        "            self._print_batch_summary(all_metrics)\n",
        "\n",
        "        return all_metrics\n",
        "\n",
        "    def _aggregate_slice_metrics(self, slice_metrics: List[dict], patient_name: str) -> dict:\n",
        "        \"\"\"\n",
        "        Aggregate slice-level metrics to patient-level metrics.\n",
        "\n",
        "        Args:\n",
        "            slice_metrics: List of metrics for each slice\n",
        "            patient_name: Name of the patient\n",
        "\n",
        "        Returns:\n",
        "            Aggregated patient-level metrics\n",
        "        \"\"\"\n",
        "        if not slice_metrics:\n",
        "            return {}\n",
        "\n",
        "        # Initialize aggregated metrics\n",
        "        patient_metrics = {\n",
        "            'patient_name': patient_name,\n",
        "            'num_slices': len(slice_metrics),\n",
        "            'avg_iou': 0.0,\n",
        "            'avg_dice_score': 0.0,\n",
        "            'avg_precision': 0.0,\n",
        "            'avg_recall': 0.0,\n",
        "            'avg_accuracy': 0.0\n",
        "        }\n",
        "\n",
        "        # Aggregate metrics across slices\n",
        "        metric_keys = ['iou', 'dice_score', 'precision', 'recall', 'accuracy']\n",
        "\n",
        "        for key in metric_keys:\n",
        "            values = [metrics.get(key, 0) for metrics in slice_metrics if key in metrics]\n",
        "            if values:\n",
        "                patient_metrics[f'avg_{key}'] = np.mean(values)\n",
        "                patient_metrics[f'std_{key}'] = np.std(values)\n",
        "                patient_metrics[f'min_{key}'] = np.min(values)\n",
        "                patient_metrics[f'max_{key}'] = np.max(values)\n",
        "\n",
        "        # Aggregate per-class metrics\n",
        "        for class_label in [0, 1, 2, 4]:\n",
        "            class_name = {0: 'Background', 1: 'Necrotic', 2: 'Edema', 4: 'Enhancing'}[class_label]\n",
        "\n",
        "            for metric_type in ['iou', 'dice_score']:\n",
        "                key = f'class_{class_label}_{metric_type}'\n",
        "                values = [metrics.get(key, 0) for metrics in slice_metrics if key in metrics]\n",
        "                if values:\n",
        "                    patient_metrics[f'avg_{key}'] = np.mean(values)\n",
        "                    patient_metrics[f'std_{key}'] = np.std(values)\n",
        "\n",
        "        # Aggregate class distribution\n",
        "        for class_label in [0, 1, 2, 4]:\n",
        "            pred_key = f'pred_class_{class_label}_count'\n",
        "            gt_key = f'gt_class_{class_label}_count'\n",
        "\n",
        "            pred_values = [metrics.get(pred_key, 0) for metrics in slice_metrics if pred_key in metrics]\n",
        "            gt_values = [metrics.get(gt_key, 0) for metrics in slice_metrics if gt_key in metrics]\n",
        "\n",
        "            if pred_values and gt_values:\n",
        "                patient_metrics[f'avg_{pred_key}'] = np.mean(pred_values)\n",
        "                patient_metrics[f'avg_{gt_key}'] = np.mean(gt_values)\n",
        "                patient_metrics[f'total_{pred_key}'] = np.sum(pred_values)\n",
        "                patient_metrics[f'total_{gt_key}'] = np.sum(gt_values)\n",
        "\n",
        "        return patient_metrics\n",
        "\n",
        "    def _convert_to_brats_labels(self, prediction: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Convert model output indices to BraTS labels.\n",
        "\n",
        "        Args:\n",
        "            prediction: Model output with indices [0,1,2,3]\n",
        "\n",
        "        Returns:\n",
        "            BraTS labels [0,1,2,4]\n",
        "        \"\"\"\n",
        "        brats_prediction = prediction.copy()\n",
        "        # Map model indices to BraTS labels\n",
        "        brats_mapping = {0: 0, 1: 1, 2: 2, 3: 4}  # model index -> BraTS label\n",
        "        for model_idx, brats_label in brats_mapping.items():\n",
        "            brats_prediction[prediction == model_idx] = brats_label\n",
        "\n",
        "        return brats_prediction.astype(np.uint8)\n",
        "\n",
        "    def _calculate_detailed_metrics(self, pred_brats: np.ndarray, gt_brats: np.ndarray) -> dict:\n",
        "        \"\"\"\n",
        "        Calculate detailed per-class metrics for BraTS labels.\n",
        "\n",
        "        Args:\n",
        "            pred_brats: Predicted BraTS labels [0,1,2,4]\n",
        "            gt_brats: Ground truth BraTS labels [0,1,2,4]\n",
        "\n",
        "        Returns:\n",
        "            Dictionary of detailed metrics\n",
        "        \"\"\"\n",
        "        sample_metrics = {}\n",
        "\n",
        "        # Overall binary metrics (tumor vs background)\n",
        "        pred_binary = (pred_brats > 0).astype(np.uint8)\n",
        "        gt_binary = (gt_brats > 0).astype(np.uint8)\n",
        "        binary_metrics = calculate_segmentation_metrics(pred_binary, gt_binary)\n",
        "        sample_metrics.update(binary_metrics)\n",
        "\n",
        "        # Per-class metrics for each BraTS label\n",
        "        for class_label in [0, 1, 2, 4]:  # BraTS labels\n",
        "            pred_class = (pred_brats == class_label).astype(np.uint8)\n",
        "            gt_class = (gt_brats == class_label).astype(np.uint8)\n",
        "\n",
        "            if gt_class.sum() > 0:  # Only calculate if class exists in ground truth\n",
        "                class_metrics = calculate_segmentation_metrics(pred_class, gt_class)\n",
        "                for metric_name, value in class_metrics.items():\n",
        "                    sample_metrics[f'class_{class_label}_{metric_name}'] = value\n",
        "\n",
        "        # Class distribution analysis\n",
        "        unique_pred, pred_counts = np.unique(pred_brats, return_counts=True)\n",
        "        unique_gt, gt_counts = np.unique(gt_brats, return_counts=True)\n",
        "\n",
        "        sample_metrics['pred_class_distribution'] = dict(zip(unique_pred, pred_counts))\n",
        "        sample_metrics['gt_class_distribution'] = dict(zip(unique_gt, gt_counts))\n",
        "\n",
        "        return sample_metrics\n",
        "\n",
        "    def _print_batch_summary(self, all_metrics: List[dict]):\n",
        "        \"\"\"\n",
        "        Print summary statistics for batch inference results.\n",
        "\n",
        "        Args:\n",
        "            all_metrics: List of patient-level metrics dictionaries from batch inference\n",
        "        \"\"\"\n",
        "        if not all_metrics:\n",
        "            print(\"No metrics to summarize\")\n",
        "            return\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"BATCH INFERENCE SUMMARY (3D VOLUMES)\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"Total patients processed: {len(all_metrics)}\")\n",
        "\n",
        "        # Calculate average metrics across patients\n",
        "        avg_metrics = {}\n",
        "        metric_keys = set()\n",
        "        for patient_metrics in all_metrics:\n",
        "            metric_keys.update(patient_metrics.keys())\n",
        "\n",
        "        for key in metric_keys:\n",
        "            if key in ['patient_name', 'num_slices']:\n",
        "                continue\n",
        "\n",
        "            values = []\n",
        "            for patient_metrics in all_metrics:\n",
        "                if key in patient_metrics and isinstance(patient_metrics[key], (int, float)):\n",
        "                    values.append(patient_metrics[key])\n",
        "\n",
        "            if values:\n",
        "                avg_metrics[f'avg_{key}'] = np.mean(values)\n",
        "                avg_metrics[f'std_{key}'] = np.std(values)\n",
        "\n",
        "        # Print overall metrics\n",
        "        print(f\"\\nOverall Performance:\")\n",
        "        print(f\"  IoU: {avg_metrics.get('avg_iou', 0):.4f} ± {avg_metrics.get('std_iou', 0):.4f}\")\n",
        "        print(f\"  Dice: {avg_metrics.get('avg_dice_score', 0):.4f} ± {avg_metrics.get('std_dice_score', 0):.4f}\")\n",
        "        print(f\"  Precision: {avg_metrics.get('avg_precision', 0):.4f} ± {avg_metrics.get('std_precision', 0):.4f}\")\n",
        "        print(f\"  Recall: {avg_metrics.get('avg_recall', 0):.4f} ± {avg_metrics.get('std_recall', 0):.4f}\")\n",
        "\n",
        "        # Print per-class metrics\n",
        "        print(f\"\\nPer-class Performance:\")\n",
        "        for class_label in [0, 1, 2, 4]:\n",
        "            class_name = {0: 'Background', 1: 'Necrotic', 2: 'Edema', 4: 'Enhancing'}[class_label]\n",
        "            iou_key = f'avg_class_{class_label}_iou'\n",
        "            dice_key = f'avg_class_{class_label}_dice_score'\n",
        "            if iou_key in avg_metrics:\n",
        "                print(f\"  {class_name} (label {class_label}):\")\n",
        "                print(f\"    IoU: {avg_metrics[iou_key]:.4f} ± {avg_metrics.get(f'std_class_{class_label}_iou', 0):.4f}\")\n",
        "                print(f\"    Dice: {avg_metrics[dice_key]:.4f} ± {avg_metrics.get(f'std_class_{class_label}_dice_score', 0):.4f}\")\n",
        "\n",
        "        print(f\"{'='*60}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xm3rW5bRfETY"
      },
      "outputs": [],
      "source": [
        "for save_dir in save_dirs:\n",
        "    print(f\"Loading model from {save_dir}\")\n",
        "    model_path=os.path.join(save_dir, 'unet_brats_final.pth')\n",
        "    output_path=os.path.join(save_dir, 'inferences')\n",
        "    inferer = UNetInference(model_path)\n",
        "    inference_metrics = inferer.batch_inference(test_dataset, output_path)\n",
        "\n",
        "print(\"Inference completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abPfUWdS9VrP"
      },
      "outputs": [],
      "source": [
        "def visualize_mri(img_file_path: str, bg_img_file_path: str, title: str):\n",
        "  \"\"\"\n",
        "   visualize mri images with segmentation masks\n",
        "\n",
        "   Args:\n",
        "    img_file_path: path to the mri image\n",
        "    bg_img_file_path: path to the background image\n",
        "    title: title of the plot\n",
        "  \"\"\"\n",
        "  mri_img = nib.load(img_file_path)\n",
        "  bg_img = nib.load(bg_img_file_path)\n",
        "  plotting.plot_roi(mri_img, bg_img = bg_img, title=title)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FExcdUgz9bL4"
      },
      "outputs": [],
      "source": [
        "assert(len(gnd_truth_masks) == len(img_files))\n",
        "assert(len(pred_masks) == len(img_files))\n",
        "\n",
        "for i in range(5):\n",
        "  random_num = random.randint(0, len(pred_masks))\n",
        "  # print(f\"image = {img_files[random_num]}\")\n",
        "  # print(f\"gnd truth shape = {nib.load(gnd_truth_masks[random_num]).get_fdata().shape}\")\n",
        "  # print(f\"pred shape = {nib.load(pred_masks[random_num]).get_fdata().shape}\")\n",
        "  # print(f\"gnd truth header = {nib.load(gnd_truth_masks[random_num]).header['dim']}\")\n",
        "  # print(f\"pred header = {nib.load(pred_masks[random_num]).header['dim']}\")\n",
        "  visualize_mri(pred_masks[random_num], img_files[random_num],title='Predicted Mask')\n",
        "  visualize_mri(gnd_truth_masks[random_num], img_files[random_num], title = 'Ground Truth Mask')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyOXswW3n77K4JQPp0xbldX5",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}